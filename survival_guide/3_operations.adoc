= Fusion 5 Survival Guide: Day Two Operations
:toc:
:toclevels: 3
:toc-title:

// tag::body[]

This topic provides an overview of common daily maintenance tasks.

=== Helm upgrade script

// tag::upgrade-script[]

Once you
ifdef::env-github[]
link:2_planning.adoc[deploy a working cluster],
endif::[]
ifndef::env-github[]
link:/how-to/deploy-fusion-at-scale.html[deploy a working cluster],
endif::[]
we recommend using the upgrade script created by the `customize_fusion_values.sh` script. The upgrade script hard-codes the various parameters and alleviates the need to remember which parameters to pass to the script. This is especially helpful when working with multiple K8s clusters. Make sure you check the script into version control alongside your custom values YAML files.

Whenever you make a change to one of the custom values yaml files for your cluster, you need to run the upgrade script to apply the changes. The script simply calls `helm upgrade` with the correct parameters and `--values` options. Remember that if you run `helm upgrade` without passing the custom values yaml files, the deployment will revert back to using chart defaults, which you never want to do.

TIP: The script assumes your kubeconfig is pointing to the correct cluster; if not, it fails fast. Be sure to select the correct kubeconfig before running the script. The script also assumes the use of Helm v3.

// end::upgrade-script[]

=== Logging

// tag::logging[]

Fusion services come pre-configured to write log messages to stdout (common pattern in K8s) and index log messages into the Fusion `system_logs` collection via a pulsar message queue processed by the `fusion-log-forwarder` component.

The Log Viewer integrated into the Fusion Admin UI provides basic log analytics for the logs collected in `system_logs`.  You can also use your preferred log analytics stack, such as ELK, Splunk, DataDog, or similar.

For integration with an existing log infrastructure, you have two basic choices:

==== Disable Log forwarding and scrape logs from stdout

. In your custom values YAML files, set
```
global:
  logging:
    disablePulsar: true
```
. Configure your logging infrastructure to scrape logs from the stdout from each pod.

It is possible to setup fusion to output json formatted logs to stdout to ease configuration of log forwarders, to enable this add the following to the values file:
```
global:
  logging:
    jsonOutput: true
```
+
This is a very common pattern in Kubernetes and most modern log analytics solutions have good integration with Kubernetes.
//In most cases, the customers ops team will help guide you on how they want this to work (typically with a log shipper process deployed as a DaemonSet on each node), there’s not much you’ll have to do.

==== Use Logstash to send logs

This is a good option if you use ELK but don't have an existing integration with Kubernetes. Fusion can be configured to send logs directly to a user's already configured logstash instance by setting the following in your values file:
```
global:
  logging:
    logstashHost: <logstash_host>
```
// This will not forward logs from solr/zookeeper/pulsar etc, so using something like fluentd is the preferred option

// end::logging[]

=== Grafana dashboards

// tag::grafana[]

If you're running Prometheus and Grafana for monitoring the performance and health of Fusion services, then you'll want to install Lucidworks Grafana dashboards. We provide a number of dashboards in the link:https://github.com/lucidworks/fusion-cloud-native/tree/master/monitoring/grafana[fusion-cloud-native repo^].

.How to configure Grafana

. If you used the `install_prom.sh` script to install Prometheus / Grafana into your Fusion namespace, then you need to get the initial Grafana password from a K8s secret by doing:
+
[source,bash]
----
kubectl get secret --namespace "${NAMESPACE}" ${RELEASE}-monitoring-grafana \
  -o jsonpath="{.data.admin-password}" | base64 --decode ; echo
----
To see the name of the Grafana deployment, do: `kubectl get deploy`
+
TIP: Replace `${RELEASE}` with your Helm release label, which is typically the same as your `${NAMESPACE}` value.

. With Grafana, you can either set up a temporary port-forward to a Grafana pod or expose Grafana on an external IP using a K8s `LoadBalancer`. To define a `LoadBalancer`, do:
+
[source,bash]
----
kubectl expose deployment ${RELEASE}-monitoring-grafana --type=LoadBalancer --name=grafana --port=3000 --target-port=3000
----

. Use `kubectl get services --namespace <namespace>` to verify that the load balancer is set up and has an external IP address.

. Direct your browser to `\http://<GrafanaIP>:3000` and enter the username `admin@localhost` and the password that was returned in the previous step.
+
This will log you into the application. It is recommended that you create another administrative user with a more desirable password.
+
If you used the `install_prom.sh` script, then a default Prometheus data source will already be configured for you.
If not, then you need to configure the Prometheus data source in Grafana.

. Go to the gear icon on the left and then go to *Data Sources*.

. Click *Add Data Source* and then click on *Prometheus* as the data source type. It will bring you to a page where it will ask for HTTP URL for the Prometheus server.
+
image:https://github.com/lucidworks/fusion-cloud-native/blob/master/survival_guide/grafana-add-datasource.png?raw=true[]
. Enter `\http://<RELEASE>-prom-prometheus-server`

. Configure any additional fields as desired (but defaults are fine), then click *Save and Test*, which should succeed.

. If you used the `install_prom.sh` script, then Fusion's default Grafana dashboards will already be imported. If not, import the dashboards from the fusion-cloud-native repo:
+
image:https://github.com/lucidworks/fusion-cloud-native/blob/master/survival_guide/grafana-import.png?raw=true[]
// end::grafana[]

=== Resource limits

// tag::resources[]

Lucidworks recommends installing Fusion without resource limits initially as they can over-complicate the initial setup of your cluster, especially for proof-of-concept / getting started clusters. Resource requests / limits directly impact the number of nodes needed to deploy Fusion. Once your installation is up and running with a critical mass of data, then you can start to fine-tune resource limits for Fusion services.

For production like environments, you should define resource limits to help K8s schedule pods correctly across the nodes in your cluster. This is especially important for K8s clusters that host other namespaces besides Fusion.

If you used the `--with-resource-limits` option when running the `./customize_fusion_values.sh` script, then you already have resource limits configured for your cluster.

Look for a file named `<provider>_<cluster>_<namespace>_fusion_resources.yaml`; if you do not have this file, simply copy https://github.com/lucidworks/fusion-cloud-native/blob/master/example-values/resources.yaml[`resources.yaml`] to help you get started with setting the appropriate resource limits.

You can refine the resource requests / limits as you test your cluster's behavior while preparing to go to production with Fusion.

// end::resources[]

=== Pod affinity rules

// tag::affinity[]

Affinity rules govern how pods for Fusion components are scheduled across the cluster. All components have the same affinity setup which follows this logic:

* When scheduling, prefer to put a pod on a node that is in an availability zone that doesn't already have a running instance of this component.

* Require that pods are all deployed on a host that doesn't have a running instance of the component that is being scheduled.

This means that the loss of a host will bring down at most one component. However, the cluster will need to be at least as large as the number of replicas in the largest deployment.

If you need to run a large number of a certain type of component, then consider relaxing the "required" policy by changing it to a "preferred" policy on hostname by changing

----
     requiredDuringSchedulingIgnoredDuringExecution:
----
to
----
     preferredDuringSchedulingIgnoredDuringExecution:
----

for the `kubernetes.io/hostname` policies.

If you used the `--with-affinity-rules` option when running the `./customize_fusion_values.sh` script, then you already have pod affinity rules configured for your cluster. If not, then we recommend copying the https://github.com/lucidworks/fusion-cloud-native/blob/master/example-values/affinity.yaml[`example-values/affinity.yaml` file^] and renaming it using our convention: `<provider>_<cluster>_<release>_fusion_affinity.yaml`.

Append the following to your
ifdef::env-github[]
link:#helm-upgrade-script[upgrade script]:
endif::[]
ifndef::env-github[]
link:/how-to/create-helm-upgrade-script.html[upgrade script]:
endif::[]

----
MY_VALUES="${MY_VALUES} --values gke_search_f5_fusion_affinity.yaml"
----

// end::affinity[]

=== Multiple replicas and horizontal pod auto-scaling

// tag::auto-scaling[]

You can configure multiple replicas and horizontal pod autoscalers (tied to CPU usage) for Fusion components.

If you used the `--with-replicas` option when running the `./customize_fusion_values.sh` script, then you already have replicas configured for your cluster.

If not, then copy the example file (`example-values/replicas.yaml`) and rename it using our convention: `<provider>_<cluster>_<release>_fusion_replicas.yaml`

Append the following to your
ifdef::env-github[]
link:#helm-upgrade-script[upgrade script]:
endif::[]
ifndef::env-github[]
link:/how-to/create-helm-upgrade-script.html[upgrade script]:
endif::[]

----
MY_VALUES="${MY_VALUES} --values gke_search_f5_fusion_replicas.yaml"
----

=== Tune Fusion Application Performance

In this section, we cover a variety of topics to help you get the best Search performance for your Fusion application.

If you have not created an application yet, proceed to the Fusion Admin UI to create your first application. For the purposes of this section, we'll use a sample application named `dcommerce`.

==== Fix Solr Collection Replica Placement

If you're using multiple Solr StatefulSets, such as to partition Solr pods into `search`, `analytics`, and `system` pools, then you need to use a Solr auto-scaling policy to govern replica placement for Fusion collections.

Open a port-forward to a Solr pod in the cluster.
----
kubectl port-forward <SOLR_POD_ID> 8983
----

Inspect the Solr auto-scaling policy in the link:https://github.com/lucidworks/fusion-cloud-native/blob/master/policy.json[policy.json^] file. The syntax is rather cryptic, but it basically defines a separate policy for search, analytics, and system oriented collections.

Run the `./update_policy.sh` script to add the Solr auto-scaling policy from policy.json into the Solr cluster.

Unfortunately, due to a limitation in Solr (https://issues.apache.org/jira/browse/SOLR-14347), replicas do not get placed correctly for Solr collections created by Fusion during application creation.

Consequently, you'll need to delete the Solr collections and re-create them using a BASH script.

The recommended approach is to adapt the link:https://github.com/lucidworks/fusion-cloud-native/blob/master/update_app_coll_layout.sh[update_app_coll_layout.sh^] script for your application, such as setting the correct number of shards, replicas, replica types, and policy for each collection used by your Fusion application.
Make a copy of the `update_app_coll_layout.sh` script and set the vars at the top for the specific app, in this case `dcommerce`.

For this example, we'll use the following settings:

[width="90%",cols="4,2,5,2",options="header"]
|=========================================================
|Collection|Shards|Replicas|Policy
|dcommerce|1|2 tlog, 3 pull|search
|dcommerce_signals_aggr|1|2 tlog, 3 pull|search
|dcommerce_query_rewrite|1|2 tlog, 3 pull|search
|dcommerce_user_prefs|1|2 nrt|search
|dcommerce_signals|3|2 nrt|analytics
|dcommerce_query_rewrite_staging|1|2 nrt|analytics
|dcommerce_job_reports|1|2 nrt|analytics
|=========================================================

Here's an example for our `dcommerce` app, adjust to meet your specific use case:
----
#!/bin/bash

APP="dcommerce"
SOLR="http://localhost:8983"

curl "$SOLR/solr/admin/collections?action=DELETE&name=${APP}"
curl "$SOLR/solr/admin/collections?action=DELETE&name=${APP}_signals"
curl "$SOLR/solr/admin/collections?action=DELETE&name=${APP}_signals_aggr"
curl "$SOLR/solr/admin/collections?action=DELETE&name=${APP}_query_rewrite_staging"
curl "$SOLR/solr/admin/collections?action=DELETE&name=${APP}_query_rewrite"
curl "$SOLR/solr/admin/collections?action=DELETE&name=${APP}_job_reports"
curl "$SOLR/solr/admin/collections?action=DELETE&name=${APP}_user_prefs"

# analytics oriented collections
curl "$SOLR/solr/admin/collections?action=CREATE&name=${APP}_signals&collection.configName=${APP}_signals&numShards=3&replicationFactor=2&policy=analytics&maxShardsPerNode=2"
curl "$SOLR/solr/admin/collections?action=CREATE&name=${APP}_query_rewrite_staging&collection.configName=${APP}_query_rewrite_staging&numShards=1&replicationFactor=2&policy=analytics"
curl "$SOLR/solr/admin/collections?action=CREATE&name=${APP}_job_reports&collection.configName=${APP}_job_reports&numShards=1&replicationFactor=2&policy=analytics"

# search oriented collections
curl "$SOLR/solr/admin/collections?action=CREATE&name=${APP}&collection.configName=${APP}&numShards=1&tlogReplicas=2&pullReplicas=3&policy=search"
curl "$SOLR/solr/admin/collections?action=CREATE&name=${APP}_signals_aggr&collection.configName=${APP}_signals_aggr&numShards=1&tlogReplicas=2&pullReplicas=3&policy=search"
curl "$SOLR/solr/admin/collections?action=CREATE&name=${APP}_query_rewrite&collection.configName=${APP}_query_rewrite&numShards=1&tlogReplicas=2&pullReplicas=3&policy=search"
curl "$SOLR/solr/admin/collections?action=CREATE&name=${APP}_user_prefs&collection.configName=${APP}_user_prefs&numShards=1&replicationFactor=2&policy=search"
----

Notice that script deletes Solr collections and re-creates them with the correct auto-scaling policy in place. Obviously, you should not run this on collections that have data without backing up the data first.

For more information about Solr replica types, see: https://lucene.apache.org/solr/guide/8_4/shards-and-indexing-data-in-solrcloud.html#types-of-replicas

==== Tune Solr Commit Settings

Fusion collections are created with a default commit within set to 10 secs. This overrides the commit settings set for a collection in the `solrconfig.xml`.

Commit within 10 seconds is too aggressive for production environments as it will cause Solr to open a new search and flush all caches.
For environments where optimal performance is important, you may want to disable the commit within setting for your collections and instead rely solely on auto soft and hard commits.

Disable commit within using the `update_commit_within_f5.sh` script, for instance:
----
./update_commit_within_f5.sh --collection dcommerce --gateway GATEWAY_URL --commit_within -1
----
Replace `GATEWAY_URL` with the URL of the K8s Ingress or IP for the Fusion API Gateway. Repeat this process for all Fusion collections.

TIP: You can get the IP of the Gateway pod using: `export LW_K8S_GATEWAY_IP=$(kubectl --namespace ${LW_K8S_NAMESPACE} get service proxy -o jsonpath='{.status.loadBalancer.ingress[0].ip}')`

Configure soft / hard auto commit settings in solrconfig.xml (via the Fusion Admin UI), such as:
----
    <autoCommit>
      <maxTime>60000</maxTime>
      <openSearcher>false</openSearcher>
    </autoCommit>

    <autoSoftCommit>
      <maxTime>300000</maxTime>
    </autoSoftCommit>
----
You want the auto soft-commit setting to be as long as possible (in millis) to avoid re-opening searchers too often, which invalidates your caches.

You should also consider disabling commits / optimize requests coming from external client applications by configuring the `IgnoreCommitOptimizeUpdateProcessorFactory` in your update processor chain(s).

----
    <processor class="solr.IgnoreCommitOptimizeUpdateProcessorFactory">
      <int name="statusCode">200</int>
      <str name="responseMessage">Thou shall not issue a commit!</str>
    </processor>
----
This prevents external client applications that you do not control from committing (or optimizing) too often. For most production environments, you should rely solely on the auto-commit settings in solrconfig.xml.

==== Enable Buffering for Index Pipelines

For each index pipeline, ensure the `Buffer Documents and Send Them to Solr in Batches` option is enabled for the Solr Index stage.

==== Tune Solr Cache Settings

Solr has a number of caches, such as the filter cache, that have a major impact on performance. For many production environments, the max size for these caches is too small and should be increased.
Be sure to look at the metrics for your caches after running load tests to determine if you need to tune them. Cache configuration is done in the solrconfig.xml for each collection using the Fusion Admin UI.

Typically the three most important caches to tune are:
----
    <filterCache class="solr.FastLRUCache"
                 size="5000"
                 maxRamMB="64"
                 autowarmCount="0"/>

    <queryResultCache class="solr.LRUCache"
                      size="6000"
                      maxRamMB="250"
                      autowarmCount="0"/>

    <documentCache class="solr.LRUCache"
                   size="25000"
                   maxRamMB="64"
                   autowarmCount="0"/>
----

TIP: Be careful with `autowarmCount` as that will impact how long it takes for a new searcher to open.

==== Query Pipeline Routing Parameters

If you're using a separate `search` pool for search oriented collections, then you'll want to add the `lw.nodeFilter=host:solr-search` parameter to the main query pipeline(s) to ensure queries get routed from Fusion to Solr Search pods only.

If you're using PULL replicas for search collections, then you should also pass `shards.preference=replica.type:PULL,replica.location:local` to Solr.

This ensures that queries get routed to PULL replicas only and favors the local replica if it exists. For more information about `shards.preference`, see:
https://lucene.apache.org/solr/guide/8_4/distributed-requests.html#shards-preference-parameter

You should also provide these parameters for sidecar queries, such as in the tagger, rules, and signals boost stages.

==== Async Query Stages

The tagger and rules stages can be configured with a max time constraint that enforces an upper bound on how long these stages can take. Behind the scenes, this requires executing the sidecar request in a background thread.

In addition, it's common to configure your pipeline to do the rules lookup and signals boost concurrently using Fusion asynchronous stage support. If you're using these features, please ensure you pass the following Java system property:
----
-Djava.util.concurrent.ForkJoinPool.common.parallelism=1
----

=== Use Gatling to run query performance / load tests

Lucidworks recommends running query performance tests to establish a baseline number of pods for the proxy, query pipeline, and Solr services. You can use the gatling-qps project provided in the link:https://github.com/lucidworks/fusion-cloud-native[fusion-cloud-native repo^] as a starting point for building a query load test. Gatling.io is a load test framework that provides a powerful Scala-based DSL for constructing performance test scenarios. See `FusionQueryTraffic.scala` in the repo as a starting point for building query performance tests for Fusion 5.

==== Register warming queries

To avoid any potential delays when a new query pod joins the cluster, such as in reaction to an HPA auto-scaling trigger, we recommend registering a small set of queries to "warm up" the query pipeline service before it gets added to the Kubernetes service. In the query-pipeline section of the custom values YAML, configure your warming queries using the structure shown in the example below:

[source,json]
----
warmingQueryJson:
  {
  "pipelines": [
    {
      "pipeline": "<PIPELINE>",
      "collection": "<COLLECTION>",
      "params": {
        "q": ["*:*"]
      }
    },{
      "method" : "POST",
      "pipeline": "<ANOTHER_PIPELINE>",
      "collection": "<ANOTHER_COLL>",
      "params": {
        "q": ["*:*"]
      }
    }
  ],
  "profiles": [
    {
      "profile": "<PROFILE>",
      "params": {
        "q": ["*:*"]
      }
    }
  ]
  }
----

NOTE: The indentation for the opening / closing braces is important for embedding JSON in YAML

==== Tune Java Heap Settings and Resource Limits

As you run query load tests, you may need to increase the Java heap settings `-Xms -Xmx` for the query pipeline service using `javaToolOptions` in the custom values YAML for your cluster.

Please ensure the memory and cpu resource limits applied to the query pipeline service align with the Java heap settings.

// end::auto-scaling[]


[[spark-ops]]
=== Spark operations

In Fusion 5.x, Spark operates in native Kubernetes mode instead of standalone mode (like in Fusion 4.x). The sections below describe Spark operations in Fusion 5.0.

// tag::spark-ops-intro[]

==== Node Selectors

You can control which nodes Spark executors are scheduled on using Spark configuration property for a job:
----
spark.kubernetes.node.selector.<LABEL>=<LABEL_VALUE>
----
For instance, if a node is labeled with `fusion_node_type=spark_only`, then you would scheduled Spark executor pods to run on that node using:
----
spark.kubernetes.node.selector.fusion_node_type=spark_only
----

TIP: Spark version 2.4.x does not support tolerations for Spark pods; consequently, Spark pods cannot be scheduled on any nodes with taints.

==== Cluster mode

Fusion 5.0 ships with Spark 2.4.3 and operates in "cluster" mode on top of Kubernetes. In cluster mode, each Spark driver runs in a separate pod and hence resources can be managed per job. Each executor also runs in its own pod.

==== Spark config defaults

The table below shows the default configurations for Spark. These settings are configured in the job-launcher config map, accessible using `kubectl get configmaps <release-name>-job-launcher`. Some of these settings are also configurable via Helm.

.Spark Resource Configurations
[cols="2m,1a,1m"]
|===
|Spark Configuration
|Default value
|Helm Variable

|spark.driver.memory
|3g
|

|spark.executor.instances
|2
|executorInstances

|spark.executor.memory
|3g
|

|spark.executor.cores
|6
|

|spark.kubernetes.executor.request.cores
|3
|

|===


.Spark Kubernetes Configurations
[cols="2m,1a,1m"]
|===
|Spark Configuration
|Default value
|Helm Variable

|spark.kubernetes.container.image.pullPolicy
|Always
|image.imagePullPolicy

|spark.kubernetes.container.image.pullSecrets
|[artifactory]
|image.imagePullSecrets

|spark.kubernetes.authenticate.driver.serviceAccountName
|<name>-job-launcher-spark
|

|spark.kubernetes.driver.container.image
|fusion-dev-docker.ci-artifactory.lucidworks.com
|image.repository

|spark.kubernetes.executor.container.image
|fusion-dev-docker.ci-artifactory.lucidworks.com
|image.repository

|===

// end::spark-ops-intro[]

==== Spark job resource allocation

//tag::spark-resources[]

===== Number of instances and cores allocated

In order to set the number of cores allocated for a job, add the following parameter keys and values in the Spark Settings field within the "advanced" job properties in the Fusion UI or the `sparkConfig` object if defining a job via the Fusion API.

If `spark.kubernetes.executor.request.cores` is not set (default config), then Spark will set the number of CPUs for the executor pod to be the same number as `spark.executor.cores`. In that case, if `spark.executor.cores` is 3, then Spark will allocate 3 CPUs for the executor pod and will run 3 tasks in parallel. To under-allocate the CPU for the executor pod and still run multiple tasks in parallel, set `spark.kubernetes.executor.request.cores` to a lower value than `spark.executor.cores`.

The ratio for `spark.kubernetes.executor.request.cores` to `spark.executor.cores` depends on the type of job: either CPU-bound or I/O-bound. Allocate more memory to the executor if more tasks are running in parallel on a single executor pod.

[cols="3m,1a"]
|===
|Parameter Key
|Example Value

|spark.executor.instances
|3

|spark.kubernetes.executor.request.cores
|3

|spark.executor.cores
|6

|spark.driver.cores
|1

|===

If these settings are left unspecified, then the job launches with a driver using one core and 3GB of memory plus two executors, each using one core with 1GB of memory.

===== Memory allocation

The amount of memory allocated to the driver and executors is controlled on a per-job basis using the `spark.executor.memory` and `spark.driver.memory` parameters in the Spark Settings section of the job definition in the Fusion UI or within the `sparkConfig` object in the JSON definition of the job.

[cols="3m,1a"]
|===
|Parameter Key
|Example Value

|spark.executor.memory
|6g

|spark.driver.memory
|2g

|===

//end::spark-resources[]

==== Configuring credentials in the Kubernetes cluster

//tag::spark-credentials[]

AWS/GCS credentials can be configured per job or per cluster.

===== Configuring GCS credentials for Spark jobs

. Create a secret containing the credentials JSON file.
+
See https://cloud.google.com/iam/docs/creating-managing-service-account-keys on how to create service account JSON files.
+
[source,bash]
----
kubectl create secret generic solr-dev-gc-serviceaccount-key --from-file=/Users/kiranchitturi/creds/solr-dev-gc-serviceaccount-key.json
----

. Create an extra config map in Kubernetes setting the required properties for GCP.
.. Create a properties file with GCP properties:
+
[source,bash]
----
$ cat gcp-launcher.properties
spark.kubernetes.driverEnv.GOOGLE_APPLICATION_CREDENTIALS = /mnt/gcp-secrets/solr-dev-gc-serviceaccount-key.json
spark.kubernetes.driver.secrets.solr-dev-gc-serviceaccount-key = /mnt/gcp-secrets
spark.kubernetes.executor.secrets.solr-dev-gc-serviceaccount-key = /mnt/gcp-secrets
spark.executorEnv.GOOGLE_APPLICATION_CREDENTIALS = /mnt/gcp-secrets/solr-dev-gc-serviceaccount-key.json
spark.hadoop.google.cloud.auth.service.account.json.keyfile = /mnt/gcp-secrets/solr-dev-gc-serviceaccount-key.json
----
.. Create a config map based on the properties file:
+
[source,bash]
----
kubectl create configmap gcp-launcher --from-file=gcp-launcher.properties
----
. Add the gcp-launcher config map to values.yaml under job-launcher:
+
[source,yaml]
----
configSources: gcp-launcher
----

===== Configuring S3 credentials for Spark jobs

AWS credentials can’t be set via a single file. So, we have to set two environment variables referring to the key and secret.

. Create a secret pointing to the creds:
+
[source,bash]
----
kubectl create secret generic aws-secret --from-literal=key='<access key>' --from-literal=secret='<secret key>'
----
. Create an extra config map in Kubernetes setting the required properties for AWS:
.. Create a properties file with AWS properties:
+
[source,bash]
----
cat aws-launcher.properties
spark.kubernetes.driver.secretKeyRef.AWS_ACCESS_KEY_ID=aws-secret:key
spark.kubernetes.driver.secretKeyRef.AWS_SECRET_ACCESS_KEY=aws-secret:secret
spark.kubernetes.executor.secretKeyRef.AWS_ACCESS_KEY_ID=aws-secret:key
spark.kubernetes.executor.secretKeyRef.AWS_SECRET_ACCESS_KEY=aws-secret:secret
----
.. Create a config map based on the properties file:
+
[source,bash]
----
kubectl create configmap aws-launcher --from-file=aws-launcher.properties
----
. Add the `aws-launcher` config map to `values.yaml` under `job-launcher`:
+
[source,yaml]
----
configSources: aws-launcher
----

===== Configuring Azure Data Lake credentials for Spark jobs

Configuring Azure through environment variables or `configMaps` does not seem to be possible at the moment. You need to manually upload the `core-site.xml` file into the job-launcher pod at `/app/spark-dist/conf`.

Currently only Data Lake Gen 1 is supported.

Here’s what the `core-site.xml` file should look like:
[source,xml]
----
<property>
  <name>dfs.adls.oauth2.access.token.provider.type</name>
  <value>ClientCredential</value>
</property>
<property>
    <name>dfs.adls.oauth2.refresh.url</name>
    <value> Insert Your OAuth 2.0 Endpoint URL Value Here </value>
</property>
<property>
    <name>dfs.adls.oauth2.client.id</name>
    <value> Insert Your Application ID Here </value>
</property>
<property>
    <name>dfs.adls.oauth2.credential</name>
    <value>Insert the Secret Key Value Here </value>
</property>
<property>
    <name>fs.adl.impl</name>
    <value>org.apache.hadoop.fs.adl.AdlFileSystem</value>
</property>
<property>
    <name>fs.AbstractFileSystem.adl.impl</name>
    <value>org.apache.hadoop.fs.adl.Adl</value>
</property>
----

===== Configuring credentials per job

. Create a Kubernetes secret with the GCP/AWS credentials.
. Add the Spark configuration to configure the secrets for the Spark driver/executor.

====== GCS

. Create a secret containing the credentials JSON file.
+
See https://cloud.google.com/iam/docs/creating-managing-service-account-keys on how to create service account JSON files.
+
[source,bash]
----
kubectl create secret generic solr-dev-gc-serviceaccount-key --from-file=/Users/kiranchitturi/creds/solr-dev-gc-serviceaccount-key.json
----
. Toggle the Advanced config in the job UI and add the following to the Spark configuration:
+
----
spark.kubernetes.driver.secrets.solr-dev-gc-serviceaccount-key = /mnt/gcp-secrets
spark.kubernetes.executor.secrets.solr-dev-gc-serviceaccount-key = /mnt/gcp-secrets
spark.kubernetes.driverEnv.GOOGLE_APPLICATION_CREDENTIALS = /mnt/gcp-secrets/solr-dev-gc-serviceaccount-key.json
spark.executorEnv.GOOGLE_APPLICATION_CREDENTIALS = /mnt/gcp-secrets/solr-dev-gc-serviceaccount-key.json
spark.hadoop.google.cloud.auth.service.account.json.keyfile = /mnt/gcp-secrets/solr-dev-gc-serviceaccount-key.json
----

====== S3

AWS credentials can’t be set via a single file. So, we have to set two environment variables referring to the key and secret.

. Create a secret pointing to the creds:
+
----
kubectl create secret generic aws-secret --from-literal=key='<access key>' --from-literal=secret='<secret key>'
----
. Toggle the Advanced config in the job UI and add the following to Spark configuration:
+
----
spark.kubernetes.driver.secretKeyRef.AWS_ACCESS_KEY_ID=aws-secret:key
spark.kubernetes.driver.secretKeyRef.AWS_SECRET_ACCESS_KEY=aws-secret:secret
spark.kubernetes.executor.secretKeyRef.AWS_ACCESS_KEY_ID=aws-secret:key
spark.kubernetes.executor.secretKeyRef.AWS_SECRET_ACCESS_KEY=aws-secret:secret
----

//end::spark-credentials[]

==== How to get logs for a Spark job

// tag::spark-logs[]

* To get the initial logs that contain information about the pod spin up, do:
+
----
curl -X GET -u admin:password123 http://localhost:8764/api/apollo/spark/driver/log/{jobId}
----
* Get the pod ID by running:
+
----
k get pods -l spark-role=driver -l jobConfigId=<job-id>
----
* Logs from failed jobs can be obtained by using:
+
----
kubectl logs [DRIVER-POD-NAME]
----
* Logs from running containers can be tailed using the `-f` parameter:
+
----
kubectl logs -f [POD-NAME]
----

Spark deletes failed and successful executor pods. Fusion provides a cleanup Kubernetes cron job that removes successfully completed driver pods every 15 minutes.

// end::spark-logs[]

==== Spark Pod Naming Restrictions

//tag::spark-pod-naming[]

Spark will generate a pod name for the running job based on the job's name and the `runId` Fusion creates to keep track of that particular instance of the job. This pod name must conform to the Kubernetes spec for pod names, which is based on the RFC-1123 for DNS. Your job name must begin and end with an alphanumeric character, only contain `-` (no underscores), and must be fewer than 63 characters in length. We recommend you stay under 30 characters, as Spark will add additional strings such as `-driver` or `-exec-1` when spinning up driver or executor pods.

//tag::spark-pod-naming[]

==== Pod cleanup

//tag::pod-cleanup[]

Spark driver pods are cleaned up using a Kubernetes cron job that runs every 15 minutes to clean up pods using this command:
----
kubectl delete pods --namespace default --field-selector=status.phase=Succeeded -l spark-role=driver
----
This cron job is created automatically when the `job-launcher` microservice is installed in the Fusion cluster.

//end::pod-cleanup[]

==== Spark History Server

//tag::history-intro[]
While logs from the Spark driver and executor pods can be viewed using `kubectl logs [POD_NAME]`, executor pods are deleted at their end of their execution, and driver pods are deleted by Fusion on a default schedule of every hour. In order to store and view Spark logs in a more long-term fashion, you can install the https://spark.apache.org/docs/latest/monitoring.html[Spark History Server^] into your Kubernetes cluster and configure Spark to write logs in a manner that will persist.
//end::history-intro[]

===== Installing Spark History Server

//tag::history-install[]

Spark History Server can be installed via its publicly-available Helm chart. To accomplish this, we must create a `values.yaml` file to configure it.
----
helm install [namespace]-spark-history-server stable/spark-history-server --values values.yaml
----

//end::history-install[]

//tag::history-config[]

===== Recommended Configuration

Our recommended configuration for using the Spark History Server with Fusion is to store and read Spark logs in cloud storage. For installations on Google Kubernetes Engine, we suggest setting these keys in the `values.yaml`:
[source,yaml]
----
gcs:
  enableGCS: true
  secret: history-secrets
  key: sparkhistory.json
  logDirectory: gs://[BUCKET_NAME]
service:
  type: ClusterIP
  port: 18080

pvc:
  enablePVC: false
nfs:
  enableExampleNFS: false
----
Note that, by default, the Spark History Server Helm chart creates an external LoadBalancer, exposing it to outside access. This is usually undesirable. In the above, we prevent this via the `service` key - the Spark History Server will only be set up on an internal IP within your cluster and will not be exposed externally. Later, we will show how to properly access the Spark History Server.

The `key` and `secret` fields provide the Spark History Server with the details of where it will find an account with access to the Google Cloud Storage bucket given in `logDirectory`. In the following example, we're going to set up a new service account that will be shared between the Spark History Server and the Spark driver/executors for both viewing and writing logs.

The `nfs.enableExampleNFS` option turns off the NFS server that the Spark History Server sets up by default, as we won't be needing it in our installation.

In order to give the Spark History Server access to the Google Cloud Storage bucket where the logs will be kept, we use `gcloud` to create a new service account, and then `keys create` to create a JSON keypair which we will shortly upload into our cluster as a Kubernetes secret.

[source,bash]
----
$ export ACCOUNT_NAME=sparkhistory
$ export GCP_PROJECT_ID=[PROJECT_ID]
$ gcloud iam service-accounts create ${ACCOUNT_NAME} --display-name "${ACCOUNT_NAME}"
$ gcloud iam service-accounts keys create "${ACCOUNT_NAME}.json" --iam-account "${
ACCOUNT_NAME}@${GCP_PROJECT_ID}.iam.gserviceaccount.com"
----

We then give our service account the `storage/admin` role, allowing it to perform create and view operations, and the final `gsutil` command applies our service account to our chosen bucket. If you have an existing service account you wish to use instead, you can skip the `create` command, though you will still need to create the JSON keypair and ensure that the existing account can read and write to the log bucket.

[source,bash]
----
$ gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} --member "serviceAccount:${ACCOUNT_NAME}@${GCP_PROJECT_ID}.iam.gserviceaccount.com" --role roles/storage.admin
$ gsutil iam ch serviceAccount:${ACCOUNT_NAME}@${GCP_PROJECT_ID}.iam.gserviceaccount.com:objectAdmin gs://[BUCKET_NAME]
----

We now need to upload the JSON keypair into the cluster as a secret:

[source,bash]
----
kubectl -n [NAMESPACE] create secret generic history-secrets --from-file=sparkhistory.json
----

With all this in place, the Spark History Server can now be installed with `helm install [namespace]-spark-history-server stable/spark-history-server --values values.yaml`.

===== Other Configurations

====== Azure

Azure is a similar process to Google Kubernetes Engine, except our logs will be stored in Azure Blob Storage, and we can either use SAS token or key access.

[source,bash]
----
$ echo "your-storage-account-name" >> azure-storage-account-name
$ echo "your-container-name" >> azure-blob-container-name
# to auth with sas token (if wasbs.sasKeyMode=true, which is the default)
$ echo "your-azure-blob-sas-key" >> azure-blob-sas-key
# or to auth with storage account key
$ echo "your-azure-storage-account-key" >> azure-storage-account-key
$ kubectl create secret generic azure-secrets --from-file=azure-storage-account-name --from-file=azure-blob-container-name [--from-file=azure-blob-sas-key | --from-file=azure-storage-account-key]
----

For SAS token access, `values.yaml` should look like:
[source,yaml]
----
wasbs:
  enableWASBS: true
  secret: azure-secrets
  sasKeyName: azure-blob-sas-key
  storageAccountNameKeyName: azure-storage-account-name
  containerKeyName: azure-blob-container-name
  logDirectory: [BUCKET_NAME]
----
For non-SAS access:
[source,yaml]
----
wasbs:
  enableWASBS: true
  secret: azure-secrets
  sasKeyMode: false
  storageAccountKeyName: azure-storage-account-key
  storageAccountNameKeyName: azure-storage-account-name
  containerKeyName:  azure-blob-container-name
  logDirectory: [BUCKET_NAME]
----

====== AWS

The recommended approach for S3 access is to use AWS IAM roles, but you can also use a access/secret key pair as a Kubernetes secret:

[source,bash]
----
$ aws iam list-access-keys --user-name your-user-name --output text | awk '{print $2}' >> aws-access-key
$ echo "your-aws-secret-key" >> aws-secret-key
$ kubectl create secret generic aws-secrets --from-file=aws-access-key --from-file=aws-secret-key
----

For IAM, your `values.yaml` will be:

[source,yaml]
----
s3:
  enableS3: true
  logDirectory: s3a://[BUCKET_NAME]
----
(Note the Hadoop `s3a://` link instead of `s3://`.)

With a access/secret pair, you’ll need to add the secret:

[source,yaml]
----
s3:
  enableS3: true
  enableIAM: false
  accessKeyName: aws-access-key
  secretKeyName: aws-secret-key
  logDirectory: s3a://[BUCKET_NAME]
----

===== Configuring Spark

After starting the Spark History Server, we must update the config map for Fusion's job-launcher so it can write logs to the same location that Spark History Server is reading from.

In this example, having installed Fusion into a namespace of `sparkhistory`, we will edit the config map to write the logs to the same Google Cloud Storage bucket we configured the Spark History Server to read from. Before editing the config map, make a copy of the existing settings in case you need to revert the changes.

[source,bash]
----
kubectl get cm -n [NAMESPACE] sparkhistory-job-launcher -o yaml > sparkhistory-job-launcher.yaml

kubectl edit cm -n [NAMESPACE] sparkhistory-job-launcher
----

Update the `spark` key with the new YAML settings below and then delete the `job-launcher` pod. The new `job-launcher` pod will apply the new configuration to subsequent jobs. In addition to the location of the secret and the settings that specify the location of the Spark eventLog, we also have to tell Spark how to access GCS with the `spark.hadoop.fs.gs.impl``spark.hadoop.fs.AbstractFileSystem.gs.impl` keys.

[source,yaml]
----
spark:
  hadoop:
    fs:
      AbstractFileSystem:
        gs:
          impl: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS
      gs:
        impl: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem
    google:
      cloud:
        auth:
          service:
            account:
              json:
                keyfile: /etc/history-secrets/[ACCOUNT_NAME].json
  eventLog:
    enabled: true
    compress: true
    dir: gs://[BUCKET_NAME]
  …
  kubernetes:
    driver:
      secrets:
        history-secrets: /etc/history-secrets
      container:
        …
    executor:
      secrets:
        history-secrets: /etc/history-secrets
      container:
        …
    …

----

//end::history-config[]

===== Accessing The Spark History Server

//tag::history-access[]

As we have set up the Spark History Server to only set up a ClusterIP, we will need to port forward the server using `kubectl`:
----
kubectl get pods -n [NAMESPACE] # to find the Spark History Server pod
kubectl port-forward [POD_NAME] -n [NAMESPACE] 18080:18080
----

You can now access the Spark History Server at `\http://localhost:18080`. Run a Spark job and confirm that you can see the logs appear in the UI.

//end::history-access[]

=== GPU Support for Deep Learning Training

// tag::gpu-support[]

In Fusion 5.3 and above, our jobs that involve training deep learning-based models (e.g. Smart Answers, Data Augmentation) will automatically use GPU resources for training if deployed on a GPU-enabled node. Your cloud provider will likely have a custom set of nodeSelectors and tolerations that will be required to map our jobs onto their GPU compute, but in the following section we will work through an example using Smart Answers and GKE.

==== Training Smart Answers on GPU with GKE

To get Smart Answers training on GPU resources within GKE, we'll firstly need to create a GPU resource within our cluster. We suggest that you create a new nodepool with a pre-emptible GPU node that will spun down when not in use. Give the nodepool a label of `node_pool:gpu`. By default, GKE will also add a taint of `nvidia.com/gpu:present=NoSchedule`, and we'll need to take that into account when updating our Helm chart's values. We'll also need to add a specific resource limit of `nvidia.com/gpu: 1 ` - again this is a GKE-specific value. Create another standard nodepool without GPU resources with a label of `node_pool:deploy` for the eventual Seldon Core deployment.

In your custom values YAML file, add:

[source,yaml]
----
question-answering:
  nodeSelector:
    default:
      cloud.google.com/gke-nodepool: gpu-nodepool
    supervised:
      seldon:
        cloud.google.com/gke-nodepool: cpu-nodepool
    coldstart:
      seldon:
        cloud.google.com/gke-nodepool: cpu-nodepool
  tolerations:
    default:
      - key: "nvidia.com/gpu"
        operator: "Equal"
        value: "present"
        effect: "NoSchedule"
    supervised:
      seldon: []
    coldstart:
      seldon: []
  resources:
    supervised:
      train:
        requests:
          nvidia.com/gpu: 1
        limits:
          nvidia.com/gpu: 1
    coldstart:
      train:
        requests:
          nvidia.com/gpu: 1
        limits:
          nvidia.com/gpu: 1
----

Note that this setup deploys all workflow steps onto the GPU node _except_ for the Seldon Core deployment. As the deployment will live on after the workflow has completed, if it assigned to the GPU node it would prevent GKE from spinning the GPU node down, increasing operating expense. 

==== Setting up Milvus on GPU with GKE

As with Training Smart Answers on GPU with GKE, setting up Milvus on GPU first requires the creation of a GPU resource/nodepool.  The setup is the same as described above.

In your custom values YAML file, at the end of the section for the ml-model-service, add a section for Milvus as shown below (assuming the name of your GPU nodepool is `gpu-nodepool`). The taints/tolerations and resource keys shown below are for a GKE-based setup and will likely vary depending on your cloud provider.

[source,yaml]
----
ml-model-service:
# ml-model-service yaml settings:
# ...
# followed by the Milvus settings:
  milvus:
    gpu:
      enabled: true
    image:
      repository: milvusdb/milvus
      tag: 0.10.2-gpu-d081520-8a2393
      pullPolicy: "IfNotPresent"
      resources:
        requests:
          nvidia.com/gpu: 1
        limits:
          nvidia.com/gpu: 1
    nodeSelector:
      cloud.google.com/gke-nodepool: gpu-nodepool
    tolerations:
      - key: "nvidia.com/gpu"
        operator: "Equal"
        value: "present"
        effect: "NoSchedule"
----

// end::gpu-support[]


=== Upgrades with Helm v3

// tag::upgrades[]

One of the most powerful features provided by Kubernetes and a cloud-native microservices architecture is the ability to do a rolling update on a live cluster. Fusion 5 allows customers to upgrade from Fusion 5.0.2 to a later 5.x.y version on a live cluster with zero downtime or disruption of service.

When Kubernetes performs a rolling update to an individual microservice, there will be a mix of old and new services in the cluster concurrently (only briefly in most cases) and requests from other services will be routed to both versions. Consequently, Lucidworks ensures all changes we make to our service do not break the API interface exposed to other services in the same 5.x line of releases. We also ensure that the stored configuration remains compatible in the same 5.x release line.

Lucidworks releases minor updates to individual services frequently, so you can can pull in those upgrades using Helm at your discretion.

.How to upgrade Fusion
. Clone the https://github.com/lucidworks/fusion-cloud-native[fusion-cloud-native repo^], if you haven't already.
. Locate the `setup_f5_<platform>.sh` script that matches your Kubernetes platform.
. Run the script with the `--upgrade` option.
+
TIP: To see what would be upgraded, pass the `--dry-run` option to the script.

The scripts in the fusion-cloud-native repo automatically pull in the latest chart updates from our Helm repository and deploy any updates needed by doing a diff of your current installation and the latest release from Lucidworks.

// end::upgrades[]

// end::body[]
